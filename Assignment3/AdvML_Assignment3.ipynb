{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "W9Zc3UYBwGd2"
   },
   "source": [
    "# Assignment #3: Write up a report using BBC text classification data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bOo6YgPqsyoV"
   },
   "source": [
    "github repo: https://github.com/SUN-Wenjun/Advanced_Machine_Learning_Assignment/tree/master/Assignment3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6dCHVWgwwOvo"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "f7K9fmn3wKcR"
   },
   "source": [
    "## load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cx34JrShvqS5"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"https://storage.googleapis.com/dataset-uploader/bbc/bbc-text.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "G6fLFUS_wdIA",
    "outputId": "094625c6-01da-419a-ab82-0571244b79fe"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>tech</td>\n",
       "      <td>tv future in the hands of viewers with home th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>business</td>\n",
       "      <td>worldcom boss  left books alone  former worldc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sport</td>\n",
       "      <td>tigers wary of farrell  gamble  leicester say ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>sport</td>\n",
       "      <td>yeading face newcastle in fa cup premiership s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>entertainment</td>\n",
       "      <td>ocean s twelve raids box office ocean s twelve...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        category                                               text\n",
       "0           tech  tv future in the hands of viewers with home th...\n",
       "1       business  worldcom boss  left books alone  former worldc...\n",
       "2          sport  tigers wary of farrell  gamble  leicester say ...\n",
       "3          sport  yeading face newcastle in fa cup premiership s...\n",
       "4  entertainment  ocean s twelve raids box office ocean s twelve..."
      ]
     },
     "execution_count": 3,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "9d8SisNw0JM5",
    "outputId": "03562243-9c07-4987-d4d3-1dd1b17be473"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2225, 2)"
      ]
     },
     "execution_count": 4,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pfJRaGm-wlwh"
   },
   "source": [
    "## Q1\n",
    "\n",
    "Visualize the categories of your target variable and describe the dataset generally (the data includes news articles from the BBC news.)  A simple description is fine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 119
    },
    "colab_type": "code",
    "id": "U9QPax1zzRaN",
    "outputId": "f016fb9a-b93a-453f-a525-09fc004d29a6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "entertainment    386\n",
       "tech             401\n",
       "politics         417\n",
       "business         510\n",
       "sport            511\n",
       "Name: category, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['category'].value_counts().sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 320
    },
    "colab_type": "code",
    "id": "HhywDXChwj3A",
    "outputId": "770d9fea-55f3-4158-ebe9-f668a3fa7785"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'Frequency')"
      ]
     },
     "execution_count": 6,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x216 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "ax = df['category'].value_counts().sort_values().plot(kind='bar',\n",
    "                                        figsize=(5,3),\n",
    "                                        title=\"Number for each Category\")\n",
    "ax.set_xlabel(\"Category Names\")\n",
    "ax.set_ylabel(\"Frequency\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aRfkJU-jzEJ7"
   },
   "source": [
    "There are firve categories of news in the dataset. Entertainment has the least number of articles, which is 386; Sport as the largest number of articles, which is 511. The dataset is generally pretty balanced in terms of catergory.             "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MQq8eQw3ziKS"
   },
   "source": [
    "## Q2\n",
    "\n",
    "Preprocess your data such that each document in the data is represented as a sequence of equal length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UOB9yJ7N0f27"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "X = df['text'].tolist()\n",
    "\n",
    "LE = LabelEncoder()\n",
    "y = LE.fit_transform(df['category'])\n",
    "y = to_categorical(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sPNq-O0d82Pj"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "m4fVyQHrywpp",
    "outputId": "d01446d4-49b3-4afb-b255-11555cd879e9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 26895 unique tokens.\n",
      "Shape of data tensor: (1780, 200)\n",
      "Shape of label tensor: (1780, 5)\n"
     ]
    }
   ],
   "source": [
    "# Tokenize the data into one hot vectors\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "\n",
    "maxlen = 200  # We will cut reviews after 200 words\n",
    "max_words = 10000  # We will only consider the top 10,000 words in the dataset\n",
    "\n",
    "tokenizer = Tokenizer(num_words=max_words)\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "sequences = tokenizer.texts_to_sequences(X_train) # converts words in each text to each word's numeric index in tokenizer dictionary.\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))\n",
    "\n",
    "data = pad_sequences(sequences, maxlen=maxlen)\n",
    "\n",
    "labels = np.asarray(y_train)\n",
    "\n",
    "print('Shape of data tensor:', data.shape)\n",
    "print('Shape of label tensor:', labels.shape)\n",
    "\n",
    "# Split the data into a training set and a validation set\n",
    "# But first, shuffle the data\n",
    "indices = np.arange(data.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "data = data[indices]\n",
    "labels = labels[indices]\n",
    "\n",
    "X_train = data\n",
    "y_train = labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "loxS4YpN-OlN"
   },
   "outputs": [],
   "source": [
    "sequences = tokenizer.texts_to_sequences(X_test)\n",
    "X_test = pad_sequences(sequences, maxlen=maxlen)\n",
    "y_test = np.asarray(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UOCdvuJe1Bdu"
   },
   "source": [
    "## Q3\n",
    "\n",
    "Use the data to fit separate models to each of the following architectures\n",
    "\n",
    "* A. A model with an embedding layer and dense layers (but w/ no layers meant for sequential data)\n",
    "\n",
    "* B. A model using an Embedding layer with Conv1d Layers\n",
    "\n",
    "* C. A model using an Embedding layer with one sequential layer (LSTM or GRU)\n",
    "\n",
    "* D. A model using an Embedding layer with stacked sequential layers (LSTM or GRU)\n",
    "\n",
    "* E. A model using an Embedding layer with bidirectional sequential layers\n",
    "\n",
    "* F. Now retrain your best model from C, D, and E using dropout (you may need to increase epochs!)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 136
    },
    "colab_type": "code",
    "id": "gMUudis-bkQw",
    "outputId": "da9c5e49-fd64-4078-be7e-14c2e6d4a9fe"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 1.],\n",
       "       [0., 0., 1., 0., 0.],\n",
       "       [1., 0., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., 1., 0.],\n",
       "       [1., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 24,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "psiaU_3Y1Pvq"
   },
   "source": [
    "### A. A model with an embedding layer and dense layers (but w/ no layers meant for sequential data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "X--H76ga025v",
    "outputId": "eaeff7e5-a63d-4f49-b65e-19c6d455e830"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_8\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_8 (Embedding)      (None, 200, 8)            80000     \n",
      "_________________________________________________________________\n",
      "flatten_8 (Flatten)          (None, 1600)              0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 5)                 8005      \n",
      "=================================================================\n",
      "Total params: 88,005\n",
      "Trainable params: 88,005\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/indexed_slices.py:434: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1424 samples, validate on 356 samples\n",
      "Epoch 1/20\n",
      "1424/1424 [==============================] - 0s 190us/step - loss: 1.5960 - acc: 0.2753 - val_loss: 1.5794 - val_acc: 0.3287\n",
      "Epoch 2/20\n",
      "1424/1424 [==============================] - 0s 128us/step - loss: 1.5142 - acc: 0.5948 - val_loss: 1.5152 - val_acc: 0.5084\n",
      "Epoch 3/20\n",
      "1424/1424 [==============================] - 0s 134us/step - loss: 1.3639 - acc: 0.7521 - val_loss: 1.3590 - val_acc: 0.6264\n",
      "Epoch 4/20\n",
      "1424/1424 [==============================] - 0s 124us/step - loss: 1.0871 - acc: 0.8287 - val_loss: 1.1083 - val_acc: 0.6910\n",
      "Epoch 5/20\n",
      "1424/1424 [==============================] - 0s 120us/step - loss: 0.7745 - acc: 0.8947 - val_loss: 0.8922 - val_acc: 0.7472\n",
      "Epoch 6/20\n",
      "1424/1424 [==============================] - 0s 124us/step - loss: 0.5301 - acc: 0.9417 - val_loss: 0.7431 - val_acc: 0.8034\n",
      "Epoch 7/20\n",
      "1424/1424 [==============================] - 0s 138us/step - loss: 0.3676 - acc: 0.9698 - val_loss: 0.6448 - val_acc: 0.8062\n",
      "Epoch 8/20\n",
      "1424/1424 [==============================] - 0s 136us/step - loss: 0.2620 - acc: 0.9860 - val_loss: 0.5766 - val_acc: 0.8230\n",
      "Epoch 9/20\n",
      "1424/1424 [==============================] - 0s 121us/step - loss: 0.1923 - acc: 0.9930 - val_loss: 0.5285 - val_acc: 0.8511\n",
      "Epoch 10/20\n",
      "1424/1424 [==============================] - 0s 123us/step - loss: 0.1434 - acc: 0.9965 - val_loss: 0.4955 - val_acc: 0.8511\n",
      "Epoch 11/20\n",
      "1424/1424 [==============================] - 0s 124us/step - loss: 0.1083 - acc: 0.9986 - val_loss: 0.4680 - val_acc: 0.8511\n",
      "Epoch 12/20\n",
      "1424/1424 [==============================] - 0s 122us/step - loss: 0.0830 - acc: 0.9986 - val_loss: 0.4473 - val_acc: 0.8511\n",
      "Epoch 13/20\n",
      "1424/1424 [==============================] - 0s 126us/step - loss: 0.0639 - acc: 0.9986 - val_loss: 0.4299 - val_acc: 0.8567\n",
      "Epoch 14/20\n",
      "1424/1424 [==============================] - 0s 128us/step - loss: 0.0496 - acc: 0.9993 - val_loss: 0.4137 - val_acc: 0.8567\n",
      "Epoch 15/20\n",
      "1424/1424 [==============================] - 0s 123us/step - loss: 0.0388 - acc: 0.9993 - val_loss: 0.4022 - val_acc: 0.8567\n",
      "Epoch 16/20\n",
      "1424/1424 [==============================] - 0s 124us/step - loss: 0.0305 - acc: 1.0000 - val_loss: 0.3924 - val_acc: 0.8596\n",
      "Epoch 17/20\n",
      "1424/1424 [==============================] - 0s 124us/step - loss: 0.0243 - acc: 1.0000 - val_loss: 0.3845 - val_acc: 0.8567\n",
      "Epoch 18/20\n",
      "1424/1424 [==============================] - 0s 126us/step - loss: 0.0196 - acc: 1.0000 - val_loss: 0.3788 - val_acc: 0.8652\n",
      "Epoch 19/20\n",
      "1424/1424 [==============================] - 0s 120us/step - loss: 0.0160 - acc: 1.0000 - val_loss: 0.3728 - val_acc: 0.8624\n",
      "Epoch 20/20\n",
      "1424/1424 [==============================] - 0s 130us/step - loss: 0.0133 - acc: 1.0000 - val_loss: 0.3703 - val_acc: 0.8624\n"
     ]
    }
   ],
   "source": [
    "# Let's start with a model that ignores the sequential steps that make up each observation\n",
    "from keras.layers import Dense, Embedding\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Flatten, Dense\n",
    "\n",
    "model = Sequential()\n",
    "# Specify the size of your vocabulary (i.e.-10,000 terms)\n",
    "# Specify the number of features you want to extract via fitting weights to your embedding matrix.\n",
    "# We also specify the maximum input length to our Embedding layer\n",
    "# so we can later flatten the embedded inputs \n",
    "model.add(Embedding(10000, 8, input_length=maxlen))\n",
    "# After the Embedding layer, \n",
    "# our activations have shape `(samples, maxlen, 8)`.\n",
    "\n",
    "# We flatten the 3D tensor of embeddings \n",
    "# into a 2D tensor of shape `(samples, maxlen * 8)`\n",
    "model.add(Flatten())\n",
    "\n",
    "# We add the classifier on top\n",
    "model.add(Dense(5, activation='softmax'))\n",
    "model.compile(optimizer='adadelta', loss='categorical_crossentropy', metrics=['acc'])\n",
    "model.summary()\n",
    "\n",
    "\n",
    "history = model.fit(X_train, y_train,\n",
    "                    epochs=20,\n",
    "                    batch_size=32,\n",
    "                    validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "4M12yINe-HR3",
    "outputId": "140260b0-5a7e-4ae5-edac-b0edde901abc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "445/445 [==============================] - 0s 89us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.4546651328547617, 0.8382022380828857]"
      ]
     },
     "execution_count": 27,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "l3J3GvP83E6J"
   },
   "source": [
    "### B. A model using an Embedding layer with Conv1d Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "hHKU_FoB15XX",
    "outputId": "54015af3-764c-48fe-9b9f-f8746e6f92f0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_11\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_11 (Embedding)     (None, 200, 128)          1280000   \n",
      "_________________________________________________________________\n",
      "conv1d_5 (Conv1D)            (None, 194, 32)           28704     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1 (None, 38, 32)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_6 (Conv1D)            (None, 32, 32)            7200      \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_3 (Glob (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 5)                 165       \n",
      "=================================================================\n",
      "Total params: 1,316,069\n",
      "Trainable params: 1,316,069\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/indexed_slices.py:434: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1424 samples, validate on 356 samples\n",
      "Epoch 1/20\n",
      "1424/1424 [==============================] - 5s 3ms/step - loss: 1.6096 - acc: 0.1945 - val_loss: 1.6046 - val_acc: 0.2669\n",
      "Epoch 2/20\n",
      "1424/1424 [==============================] - 0s 59us/step - loss: 1.5937 - acc: 0.3336 - val_loss: 1.6014 - val_acc: 0.2725\n",
      "Epoch 3/20\n",
      "1424/1424 [==============================] - 0s 54us/step - loss: 1.5838 - acc: 0.3764 - val_loss: 1.5987 - val_acc: 0.2837\n",
      "Epoch 4/20\n",
      "1424/1424 [==============================] - 0s 58us/step - loss: 1.5752 - acc: 0.3912 - val_loss: 1.5963 - val_acc: 0.2725\n",
      "Epoch 5/20\n",
      "1424/1424 [==============================] - 0s 54us/step - loss: 1.5670 - acc: 0.4431 - val_loss: 1.5943 - val_acc: 0.3090\n",
      "Epoch 6/20\n",
      "1424/1424 [==============================] - 0s 57us/step - loss: 1.5596 - acc: 0.4501 - val_loss: 1.5928 - val_acc: 0.2893\n",
      "Epoch 7/20\n",
      "1424/1424 [==============================] - 0s 54us/step - loss: 1.5524 - acc: 0.4544 - val_loss: 1.5910 - val_acc: 0.2865\n",
      "Epoch 8/20\n",
      "1424/1424 [==============================] - 0s 56us/step - loss: 1.5454 - acc: 0.4586 - val_loss: 1.5890 - val_acc: 0.2612\n",
      "Epoch 9/20\n",
      "1424/1424 [==============================] - 0s 56us/step - loss: 1.5384 - acc: 0.4558 - val_loss: 1.5874 - val_acc: 0.2360\n",
      "Epoch 10/20\n",
      "1424/1424 [==============================] - 0s 55us/step - loss: 1.5315 - acc: 0.4529 - val_loss: 1.5858 - val_acc: 0.2303\n",
      "Epoch 11/20\n",
      "1424/1424 [==============================] - 0s 53us/step - loss: 1.5248 - acc: 0.4586 - val_loss: 1.5840 - val_acc: 0.2416\n",
      "Epoch 12/20\n",
      "1424/1424 [==============================] - 0s 54us/step - loss: 1.5182 - acc: 0.4614 - val_loss: 1.5828 - val_acc: 0.2275\n",
      "Epoch 13/20\n",
      "1424/1424 [==============================] - 0s 56us/step - loss: 1.5115 - acc: 0.4600 - val_loss: 1.5810 - val_acc: 0.2275\n",
      "Epoch 14/20\n",
      "1424/1424 [==============================] - 0s 55us/step - loss: 1.5048 - acc: 0.4579 - val_loss: 1.5793 - val_acc: 0.2331\n",
      "Epoch 15/20\n",
      "1424/1424 [==============================] - 0s 54us/step - loss: 1.4980 - acc: 0.4649 - val_loss: 1.5776 - val_acc: 0.2303\n",
      "Epoch 16/20\n",
      "1424/1424 [==============================] - 0s 58us/step - loss: 1.4910 - acc: 0.4670 - val_loss: 1.5760 - val_acc: 0.2275\n",
      "Epoch 17/20\n",
      "1424/1424 [==============================] - 0s 54us/step - loss: 1.4841 - acc: 0.4698 - val_loss: 1.5737 - val_acc: 0.2556\n",
      "Epoch 18/20\n",
      "1424/1424 [==============================] - 0s 61us/step - loss: 1.4768 - acc: 0.4726 - val_loss: 1.5721 - val_acc: 0.2275\n",
      "Epoch 19/20\n",
      "1424/1424 [==============================] - 0s 57us/step - loss: 1.4695 - acc: 0.4782 - val_loss: 1.5700 - val_acc: 0.2275\n",
      "Epoch 20/20\n",
      "1424/1424 [==============================] - 0s 58us/step - loss: 1.4622 - acc: 0.4754 - val_loss: 1.5672 - val_acc: 0.2893\n"
     ]
    }
   ],
   "source": [
    "# Use 1D Conv layer rather than RNN or LSTM or GRU to fit model\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras import layers\n",
    "from keras.optimizers import RMSprop\n",
    "\n",
    "model = Sequential()\n",
    "model.add(layers.Embedding(max_words, 128, input_length=maxlen))\n",
    "model.add(layers.Conv1D(32, 7, activation='relu')) \n",
    "model.add(layers.MaxPooling1D(5)) #\n",
    "model.add(layers.Conv1D(32, 7, activation='relu'))\n",
    "model.add(layers.GlobalMaxPooling1D())\n",
    "model.add(layers.Dense(5, activation='softmax'))\n",
    "\n",
    "model.summary()\n",
    "\n",
    "model.compile(optimizer=RMSprop(lr=1e-4),\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['acc'])\n",
    "history = model.fit(X_train, y_train,\n",
    "                    epochs=20,\n",
    "                    batch_size=128,\n",
    "                    validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "4GrOfwxp_FAD",
    "outputId": "b7185c45-baf4-4fd1-bcf5-ebe15310ed8d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "445/445 [==============================] - 0s 190us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.5781354644325343, 0.2943820357322693]"
      ]
     },
     "execution_count": 31,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Tp6tiiHn3Z9O"
   },
   "source": [
    "### C. A model using an Embedding layer with one sequential layer (LSTM or GRU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 768
    },
    "colab_type": "code",
    "id": "cab_1otv3cXd",
    "outputId": "88f111e3-794f-4ac1-9399-c3f8e7a8b535"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/indexed_slices.py:434: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1424 samples, validate on 356 samples\n",
      "Epoch 1/20\n",
      "1424/1424 [==============================] - 9s 6ms/step - loss: 1.6101 - acc: 0.2163 - val_loss: 1.5936 - val_acc: 0.2725\n",
      "Epoch 2/20\n",
      "1424/1424 [==============================] - 8s 6ms/step - loss: 1.5089 - acc: 0.4031 - val_loss: 1.5814 - val_acc: 0.2640\n",
      "Epoch 3/20\n",
      "1424/1424 [==============================] - 8s 6ms/step - loss: 1.3428 - acc: 0.5927 - val_loss: 1.5553 - val_acc: 0.3174\n",
      "Epoch 4/20\n",
      "1424/1424 [==============================] - 9s 6ms/step - loss: 1.0957 - acc: 0.7233 - val_loss: 1.6121 - val_acc: 0.3287\n",
      "Epoch 5/20\n",
      "1424/1424 [==============================] - 9s 6ms/step - loss: 0.7717 - acc: 0.8624 - val_loss: 1.6147 - val_acc: 0.3146\n",
      "Epoch 6/20\n",
      "1424/1424 [==============================] - 8s 6ms/step - loss: 0.4477 - acc: 0.9579 - val_loss: 1.6909 - val_acc: 0.3455\n",
      "Epoch 7/20\n",
      "1424/1424 [==============================] - 9s 6ms/step - loss: 0.2140 - acc: 0.9937 - val_loss: 1.7598 - val_acc: 0.3511\n",
      "Epoch 8/20\n",
      "1424/1424 [==============================] - 8s 6ms/step - loss: 0.0958 - acc: 1.0000 - val_loss: 1.8784 - val_acc: 0.3511\n",
      "Epoch 9/20\n",
      "1424/1424 [==============================] - 9s 6ms/step - loss: 0.0458 - acc: 1.0000 - val_loss: 1.9464 - val_acc: 0.3287\n",
      "Epoch 10/20\n",
      "1424/1424 [==============================] - 8s 6ms/step - loss: 0.0252 - acc: 1.0000 - val_loss: 2.0223 - val_acc: 0.3287\n",
      "Epoch 11/20\n",
      "1424/1424 [==============================] - 9s 6ms/step - loss: 0.0155 - acc: 1.0000 - val_loss: 2.1035 - val_acc: 0.3371\n",
      "Epoch 12/20\n",
      "1424/1424 [==============================] - 9s 6ms/step - loss: 0.0113 - acc: 1.0000 - val_loss: 2.0751 - val_acc: 0.3427\n",
      "Epoch 13/20\n",
      "1424/1424 [==============================] - 9s 6ms/step - loss: 0.0069 - acc: 1.0000 - val_loss: 2.2108 - val_acc: 0.3315\n",
      "Epoch 14/20\n",
      "1424/1424 [==============================] - 9s 6ms/step - loss: 0.0045 - acc: 1.0000 - val_loss: 2.2633 - val_acc: 0.3567\n",
      "Epoch 15/20\n",
      "1424/1424 [==============================] - 8s 6ms/step - loss: 0.0067 - acc: 1.0000 - val_loss: 2.2922 - val_acc: 0.3230\n",
      "Epoch 16/20\n",
      "1424/1424 [==============================] - 8s 6ms/step - loss: 0.0027 - acc: 1.0000 - val_loss: 2.3373 - val_acc: 0.3287\n",
      "Epoch 17/20\n",
      "1424/1424 [==============================] - 8s 6ms/step - loss: 0.0037 - acc: 1.0000 - val_loss: 2.4673 - val_acc: 0.3174\n",
      "Epoch 18/20\n",
      "1424/1424 [==============================] - 8s 6ms/step - loss: 1.4908 - acc: 0.5337 - val_loss: 2.4716 - val_acc: 0.2753\n",
      "Epoch 19/20\n",
      "1424/1424 [==============================] - 8s 6ms/step - loss: 1.1220 - acc: 0.5730 - val_loss: 1.9372 - val_acc: 0.3006\n",
      "Epoch 20/20\n",
      "1424/1424 [==============================] - 8s 6ms/step - loss: 0.5076 - acc: 0.8427 - val_loss: 1.8790 - val_acc: 0.3483\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import SimpleRNN, LSTM\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, SimpleRNN\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_words, 32))\n",
    "model.add(SimpleRNN(32))\n",
    "model.add(Dense(5, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='adadelta', loss='categorical_crossentropy', metrics=['acc'])\n",
    "history = model.fit(X_train, y_train,\n",
    "                    epochs=20,\n",
    "                    batch_size=32,\n",
    "                    validation_split=0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "8ysaQ4Xi_HLW",
    "outputId": "bc88a5f8-767c-4cc0-edcb-8f28f5fd4cfa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "445/445 [==============================] - 0s 574us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.8861691231138251, 0.31910112500190735]"
      ]
     },
     "execution_count": 33,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9VCxcu6Q3dA5"
   },
   "source": [
    "### D. A model using an Embedding layer with stacked sequential layers (LSTM or GRU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 768
    },
    "colab_type": "code",
    "id": "I7eYaDcx3f90",
    "outputId": "c63f2b4f-fa23-4a8b-edf3-baaac5c5027d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/indexed_slices.py:434: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1424 samples, validate on 356 samples\n",
      "Epoch 1/20\n",
      "1424/1424 [==============================] - 32s 23ms/step - loss: 1.6375 - acc: 0.2198 - val_loss: 1.6646 - val_acc: 0.1545\n",
      "Epoch 2/20\n",
      "1424/1424 [==============================] - 32s 23ms/step - loss: 1.4489 - acc: 0.3827 - val_loss: 1.5713 - val_acc: 0.2809\n",
      "Epoch 3/20\n",
      "1424/1424 [==============================] - 31s 22ms/step - loss: 1.0163 - acc: 0.6376 - val_loss: 1.6720 - val_acc: 0.3062\n",
      "Epoch 4/20\n",
      "1424/1424 [==============================] - 31s 22ms/step - loss: 0.5289 - acc: 0.8553 - val_loss: 1.9419 - val_acc: 0.3567\n",
      "Epoch 5/20\n",
      "1424/1424 [==============================] - 32s 22ms/step - loss: 0.2037 - acc: 0.9684 - val_loss: 2.2005 - val_acc: 0.3511\n",
      "Epoch 6/20\n",
      "1424/1424 [==============================] - 31s 22ms/step - loss: 0.0709 - acc: 0.9958 - val_loss: 2.3926 - val_acc: 0.3371\n",
      "Epoch 7/20\n",
      "1424/1424 [==============================] - 32s 22ms/step - loss: 0.0296 - acc: 0.9993 - val_loss: 2.5346 - val_acc: 0.3315\n",
      "Epoch 8/20\n",
      "1424/1424 [==============================] - 31s 22ms/step - loss: 0.0131 - acc: 1.0000 - val_loss: 2.7111 - val_acc: 0.3174\n",
      "Epoch 9/20\n",
      "1424/1424 [==============================] - 32s 23ms/step - loss: 0.0065 - acc: 1.0000 - val_loss: 2.8610 - val_acc: 0.3371\n",
      "Epoch 10/20\n",
      "1424/1424 [==============================] - 32s 22ms/step - loss: 0.0034 - acc: 1.0000 - val_loss: 3.0446 - val_acc: 0.3230\n",
      "Epoch 11/20\n",
      "1424/1424 [==============================] - 31s 22ms/step - loss: 0.0019 - acc: 1.0000 - val_loss: 3.2023 - val_acc: 0.3118\n",
      "Epoch 12/20\n",
      "1424/1424 [==============================] - 32s 22ms/step - loss: 0.0012 - acc: 1.0000 - val_loss: 3.3462 - val_acc: 0.3006\n",
      "Epoch 13/20\n",
      "1424/1424 [==============================] - 31s 22ms/step - loss: 8.5237e-04 - acc: 1.0000 - val_loss: 3.4441 - val_acc: 0.2978\n",
      "Epoch 14/20\n",
      "1424/1424 [==============================] - 31s 22ms/step - loss: 6.4457e-04 - acc: 1.0000 - val_loss: 3.5154 - val_acc: 0.3090\n",
      "Epoch 15/20\n",
      "1424/1424 [==============================] - 31s 22ms/step - loss: 5.1873e-04 - acc: 1.0000 - val_loss: 3.5631 - val_acc: 0.3034\n",
      "Epoch 16/20\n",
      "1424/1424 [==============================] - 32s 22ms/step - loss: 4.3045e-04 - acc: 1.0000 - val_loss: 3.6170 - val_acc: 0.3062\n",
      "Epoch 17/20\n",
      "1424/1424 [==============================] - 31s 22ms/step - loss: 3.6884e-04 - acc: 1.0000 - val_loss: 3.6650 - val_acc: 0.2978\n",
      "Epoch 18/20\n",
      "1424/1424 [==============================] - 32s 22ms/step - loss: 3.2344e-04 - acc: 1.0000 - val_loss: 3.6999 - val_acc: 0.3006\n",
      "Epoch 19/20\n",
      "1424/1424 [==============================] - 32s 22ms/step - loss: 2.8716e-04 - acc: 1.0000 - val_loss: 3.7542 - val_acc: 0.3034\n",
      "Epoch 20/20\n",
      "1424/1424 [==============================] - 32s 22ms/step - loss: 2.5825e-04 - acc: 1.0000 - val_loss: 3.7872 - val_acc: 0.3034\n"
     ]
    }
   ],
   "source": [
    "# Stacked RNN layers\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(10000, 32))\n",
    "model.add(SimpleRNN(32, return_sequences=True))\n",
    "model.add(SimpleRNN(32, return_sequences=True))\n",
    "model.add(SimpleRNN(32, return_sequences=True))\n",
    "model.add(SimpleRNN(32))\n",
    "model.add(Dense(5, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='adadelta', loss='categorical_crossentropy', metrics=['acc'])\n",
    "history = model.fit(X_train, y_train,\n",
    "                    epochs=20,\n",
    "                    batch_size=32,\n",
    "                    validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "4ARAEVat_5Xu",
    "outputId": "10cb15b0-6ff6-44a0-a01e-73a7d0abfacb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "445/445 [==============================] - 1s 2ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[4.593122086900004, 0.2247191071510315]"
      ]
     },
     "execution_count": 35,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SvwKMa1g3gwn"
   },
   "source": [
    "### E. A model using an Embedding layer with bidirectional sequential layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 768
    },
    "colab_type": "code",
    "id": "Y9I1ieX03iic",
    "outputId": "c3a6ca9f-4999-4b57-ef52-4df930a226ca"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/indexed_slices.py:434: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1424 samples, validate on 356 samples\n",
      "Epoch 1/20\n",
      "1424/1424 [==============================] - 12s 9ms/step - loss: 1.6075 - acc: 0.2219 - val_loss: 1.6045 - val_acc: 0.2163\n",
      "Epoch 2/20\n",
      "1424/1424 [==============================] - 12s 8ms/step - loss: 1.6016 - acc: 0.2409 - val_loss: 1.5981 - val_acc: 0.2388\n",
      "Epoch 3/20\n",
      "1424/1424 [==============================] - 12s 8ms/step - loss: 1.5952 - acc: 0.2823 - val_loss: 1.5921 - val_acc: 0.2640\n",
      "Epoch 4/20\n",
      "1424/1424 [==============================] - 11s 8ms/step - loss: 1.5875 - acc: 0.2879 - val_loss: 1.5857 - val_acc: 0.2921\n",
      "Epoch 5/20\n",
      "1424/1424 [==============================] - 12s 8ms/step - loss: 1.5736 - acc: 0.2816 - val_loss: 1.5738 - val_acc: 0.2781\n",
      "Epoch 6/20\n",
      "1424/1424 [==============================] - 12s 8ms/step - loss: 1.5607 - acc: 0.2992 - val_loss: 1.5628 - val_acc: 0.3174\n",
      "Epoch 7/20\n",
      "1424/1424 [==============================] - 11s 8ms/step - loss: 1.5459 - acc: 0.2942 - val_loss: 1.5613 - val_acc: 0.2949\n",
      "Epoch 8/20\n",
      "1424/1424 [==============================] - 12s 8ms/step - loss: 1.5349 - acc: 0.3139 - val_loss: 1.5448 - val_acc: 0.3315\n",
      "Epoch 9/20\n",
      "1424/1424 [==============================] - 12s 8ms/step - loss: 1.4408 - acc: 0.3996 - val_loss: 1.4520 - val_acc: 0.3624\n",
      "Epoch 10/20\n",
      "1424/1424 [==============================] - 12s 8ms/step - loss: 1.3940 - acc: 0.3848 - val_loss: 1.3782 - val_acc: 0.4382\n",
      "Epoch 11/20\n",
      "1424/1424 [==============================] - 12s 8ms/step - loss: 1.3141 - acc: 0.4087 - val_loss: 1.3353 - val_acc: 0.4213\n",
      "Epoch 12/20\n",
      "1424/1424 [==============================] - 12s 8ms/step - loss: 1.2881 - acc: 0.3961 - val_loss: 1.2689 - val_acc: 0.4719\n",
      "Epoch 13/20\n",
      "1424/1424 [==============================] - 12s 8ms/step - loss: 1.1837 - acc: 0.4944 - val_loss: 1.1738 - val_acc: 0.4803\n",
      "Epoch 14/20\n",
      "1424/1424 [==============================] - 12s 9ms/step - loss: 1.1642 - acc: 0.4831 - val_loss: 1.2202 - val_acc: 0.5225\n",
      "Epoch 15/20\n",
      "1424/1424 [==============================] - 11s 8ms/step - loss: 1.0742 - acc: 0.5407 - val_loss: 1.2847 - val_acc: 0.4691\n",
      "Epoch 16/20\n",
      "1424/1424 [==============================] - 12s 8ms/step - loss: 1.0989 - acc: 0.5625 - val_loss: 1.2927 - val_acc: 0.4860\n",
      "Epoch 17/20\n",
      "1424/1424 [==============================] - 11s 8ms/step - loss: 1.0597 - acc: 0.5162 - val_loss: 1.3626 - val_acc: 0.4242\n",
      "Epoch 18/20\n",
      "1424/1424 [==============================] - 12s 8ms/step - loss: 0.9883 - acc: 0.5976 - val_loss: 1.0957 - val_acc: 0.5000\n",
      "Epoch 19/20\n",
      "1424/1424 [==============================] - 12s 8ms/step - loss: 1.0576 - acc: 0.5428 - val_loss: 1.1298 - val_acc: 0.5843\n",
      "Epoch 20/20\n",
      "1424/1424 [==============================] - 12s 8ms/step - loss: 0.9096 - acc: 0.6166 - val_loss: 1.0033 - val_acc: 0.5506\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(layers.Embedding(max_words, 32))\n",
    "model.add(layers.Bidirectional(layers.LSTM(32)))\n",
    "model.add(layers.Dense(5, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='adadelta', loss='categorical_crossentropy', metrics=['acc'])\n",
    "history = model.fit(X_train, y_train, \n",
    "                    epochs=20, \n",
    "                    batch_size=128, \n",
    "                    validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "E6fumGmFAx5l",
    "outputId": "e73a0df4-003d-4a83-b770-c332724562c0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "445/445 [==============================] - 1s 2ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.0028762785236487, 0.550561785697937]"
      ]
     },
     "execution_count": 38,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qRrsBEZH3jL7"
   },
   "source": [
    "### F. Now retrain your best model from C, D, and E using dropout (you may need to increase epochs!)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "t7pK7cOlmRE1"
   },
   "source": [
    "Model E works best, so I will add dropout to it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "elJjCHqL3koN",
    "outputId": "4f97794b-93a6-4476-ef0c-4660169d36c8"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/indexed_slices.py:434: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1424 samples, validate on 356 samples\n",
      "Epoch 1/40\n",
      "1424/1424 [==============================] - 14s 10ms/step - loss: 1.6056 - acc: 0.2177 - val_loss: 1.6012 - val_acc: 0.2725\n",
      "Epoch 2/40\n",
      "1424/1424 [==============================] - 13s 9ms/step - loss: 1.5988 - acc: 0.2507 - val_loss: 1.5957 - val_acc: 0.2163\n",
      "Epoch 3/40\n",
      "1424/1424 [==============================] - 14s 10ms/step - loss: 1.5921 - acc: 0.2367 - val_loss: 1.5881 - val_acc: 0.3118\n",
      "Epoch 4/40\n",
      "1424/1424 [==============================] - 14s 9ms/step - loss: 1.5830 - acc: 0.2837 - val_loss: 1.5852 - val_acc: 0.2163\n",
      "Epoch 5/40\n",
      "1424/1424 [==============================] - 14s 10ms/step - loss: 1.5688 - acc: 0.2654 - val_loss: 1.5724 - val_acc: 0.2921\n",
      "Epoch 6/40\n",
      "1424/1424 [==============================] - 14s 10ms/step - loss: 1.5563 - acc: 0.2949 - val_loss: 1.5462 - val_acc: 0.3848\n",
      "Epoch 7/40\n",
      "1424/1424 [==============================] - 14s 10ms/step - loss: 1.5530 - acc: 0.3188 - val_loss: 1.5442 - val_acc: 0.3427\n",
      "Epoch 8/40\n",
      "1424/1424 [==============================] - 14s 9ms/step - loss: 1.4913 - acc: 0.3504 - val_loss: 1.5324 - val_acc: 0.3062\n",
      "Epoch 9/40\n",
      "1424/1424 [==============================] - 13s 9ms/step - loss: 1.4837 - acc: 0.3560 - val_loss: 1.5160 - val_acc: 0.3933\n",
      "Epoch 10/40\n",
      "1424/1424 [==============================] - 14s 10ms/step - loss: 1.4559 - acc: 0.3841 - val_loss: 1.4592 - val_acc: 0.3539\n",
      "Epoch 11/40\n",
      "1424/1424 [==============================] - 14s 10ms/step - loss: 1.4059 - acc: 0.3940 - val_loss: 1.3893 - val_acc: 0.3764\n",
      "Epoch 12/40\n",
      "1424/1424 [==============================] - 14s 10ms/step - loss: 1.3705 - acc: 0.4150 - val_loss: 1.4125 - val_acc: 0.4073\n",
      "Epoch 13/40\n",
      "1424/1424 [==============================] - 14s 10ms/step - loss: 1.2820 - acc: 0.4558 - val_loss: 1.3884 - val_acc: 0.3961\n",
      "Epoch 14/40\n",
      "1424/1424 [==============================] - 14s 10ms/step - loss: 1.2625 - acc: 0.4775 - val_loss: 1.2554 - val_acc: 0.5028\n",
      "Epoch 15/40\n",
      "1424/1424 [==============================] - 14s 10ms/step - loss: 1.1800 - acc: 0.5126 - val_loss: 1.3063 - val_acc: 0.4551\n",
      "Epoch 16/40\n",
      "1424/1424 [==============================] - 14s 10ms/step - loss: 1.1409 - acc: 0.5267 - val_loss: 1.0606 - val_acc: 0.6067\n",
      "Epoch 17/40\n",
      "1424/1424 [==============================] - 14s 9ms/step - loss: 1.0616 - acc: 0.5555 - val_loss: 0.9379 - val_acc: 0.6376\n",
      "Epoch 18/40\n",
      "1424/1424 [==============================] - 14s 10ms/step - loss: 1.0272 - acc: 0.5772 - val_loss: 1.1358 - val_acc: 0.5393\n",
      "Epoch 19/40\n",
      "1424/1424 [==============================] - 14s 10ms/step - loss: 1.0456 - acc: 0.5850 - val_loss: 1.2215 - val_acc: 0.5758\n",
      "Epoch 20/40\n",
      "1424/1424 [==============================] - 14s 9ms/step - loss: 1.0010 - acc: 0.6039 - val_loss: 1.0627 - val_acc: 0.5955\n",
      "Epoch 21/40\n",
      "1424/1424 [==============================] - 14s 10ms/step - loss: 0.9323 - acc: 0.6320 - val_loss: 0.9436 - val_acc: 0.6264\n",
      "Epoch 22/40\n",
      "1424/1424 [==============================] - 14s 10ms/step - loss: 0.9007 - acc: 0.6573 - val_loss: 0.8918 - val_acc: 0.6601\n",
      "Epoch 23/40\n",
      "1424/1424 [==============================] - 14s 10ms/step - loss: 0.8546 - acc: 0.6756 - val_loss: 1.0000 - val_acc: 0.6292\n",
      "Epoch 24/40\n",
      "1424/1424 [==============================] - 14s 10ms/step - loss: 0.8341 - acc: 0.7029 - val_loss: 0.9147 - val_acc: 0.6882\n",
      "Epoch 25/40\n",
      "1424/1424 [==============================] - 14s 9ms/step - loss: 0.7891 - acc: 0.7338 - val_loss: 0.8549 - val_acc: 0.6854\n",
      "Epoch 26/40\n",
      "1424/1424 [==============================] - 14s 10ms/step - loss: 0.7394 - acc: 0.7451 - val_loss: 0.8515 - val_acc: 0.7022\n",
      "Epoch 27/40\n",
      "1424/1424 [==============================] - 14s 10ms/step - loss: 0.7037 - acc: 0.7472 - val_loss: 0.9757 - val_acc: 0.6376\n",
      "Epoch 28/40\n",
      "1424/1424 [==============================] - 14s 10ms/step - loss: 0.7018 - acc: 0.7711 - val_loss: 0.7442 - val_acc: 0.7528\n",
      "Epoch 29/40\n",
      "1424/1424 [==============================] - 14s 10ms/step - loss: 0.6318 - acc: 0.8083 - val_loss: 0.7469 - val_acc: 0.7275\n",
      "Epoch 30/40\n",
      "1424/1424 [==============================] - 14s 10ms/step - loss: 0.6025 - acc: 0.8153 - val_loss: 0.7425 - val_acc: 0.7584\n",
      "Epoch 31/40\n",
      "1424/1424 [==============================] - 14s 10ms/step - loss: 0.5570 - acc: 0.8378 - val_loss: 0.7864 - val_acc: 0.7331\n",
      "Epoch 32/40\n",
      "1424/1424 [==============================] - 14s 10ms/step - loss: 0.5390 - acc: 0.8413 - val_loss: 0.8441 - val_acc: 0.7360\n",
      "Epoch 33/40\n",
      "1424/1424 [==============================] - 14s 10ms/step - loss: 0.4970 - acc: 0.8701 - val_loss: 0.7094 - val_acc: 0.7612\n",
      "Epoch 34/40\n",
      "1424/1424 [==============================] - 14s 10ms/step - loss: 0.4721 - acc: 0.8736 - val_loss: 0.7038 - val_acc: 0.7640\n",
      "Epoch 35/40\n",
      "1424/1424 [==============================] - 13s 9ms/step - loss: 0.4171 - acc: 0.8883 - val_loss: 0.6287 - val_acc: 0.8090\n",
      "Epoch 36/40\n",
      "1424/1424 [==============================] - 14s 10ms/step - loss: 0.4147 - acc: 0.8771 - val_loss: 0.6344 - val_acc: 0.7809\n",
      "Epoch 37/40\n",
      "1424/1424 [==============================] - 14s 10ms/step - loss: 0.3912 - acc: 0.8947 - val_loss: 0.6910 - val_acc: 0.7669\n",
      "Epoch 38/40\n",
      "1424/1424 [==============================] - 14s 10ms/step - loss: 0.3482 - acc: 0.9045 - val_loss: 0.6511 - val_acc: 0.7640\n",
      "Epoch 39/40\n",
      "1424/1424 [==============================] - 14s 10ms/step - loss: 0.3274 - acc: 0.9157 - val_loss: 0.6090 - val_acc: 0.8006\n",
      "Epoch 40/40\n",
      "1424/1424 [==============================] - 13s 9ms/step - loss: 0.2979 - acc: 0.9270 - val_loss: 0.5737 - val_acc: 0.8090\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(layers.Embedding(max_words, 32))\n",
    "model.add(layers.Bidirectional(layers.LSTM(32, dropout=0.2, recurrent_dropout=0.2)))\n",
    "model.add(layers.Dense(5, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='adadelta', loss='categorical_crossentropy', metrics=['acc'])\n",
    "history = model.fit(X_train, y_train, \n",
    "                    epochs=40,  # double epochs \n",
    "                    batch_size=128, \n",
    "                    validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "6J_uEc3Oo-Fu",
    "outputId": "96e15b28-5b89-49c5-adbe-4211b2f0a632"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "445/445 [==============================] - 1s 2ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.5971800802798753, 0.8044943809509277]"
      ]
     },
     "execution_count": 42,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "H5NrHoSfmmJj"
   },
   "source": [
    "## Q4 \n",
    "\n",
    "Discuss 1) which model(s) performed best and speculate about 2) how you might try to further improve the predictive power of your model (e.g. Glove embeddings? More layers? Combining Conv1D with LSTM layers? More LSTM hidden nodes?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MJ0VdRBkmsbU"
   },
   "source": [
    "I am actually pretty surprised that model A (the one with an embedding layer and dense layers but no layers meant for sequential data) turned out to preform very well too. It had an accuracy of 0.83 on the test set, higher than all other models. It worth noting that, the LSTM model with dropout also preform good - an accuracy of 0.80. \n",
    "\n",
    "As for the reason why the vanilla model prefomed so well, honestly, I am not sure. My guess is that this dataset is not very large, so more complex models may be an overfkill. The good preformance of the LSTM with dropout model also support this guess - the dropout deals with overfitting and it greatly improve the preformance. \n",
    "\n",
    "For next step, I would probably try pretrained embeddings. I know there are some pretaining embedding is built particularly for news. On the other hand, I don't think more layers, combining Conv1D with LSTM, or adding more LSTM hidden nodes. Because too complex models would be more vulnerable to over-fitting. "
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "AdvML_Assignment3.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
